# -*- coding: utf-8 -*-
"""kohya-LoRA-dreambooth.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QkiLvR-yFdasfHQDpM8lLluz_JzuU2Mj

![visitors](https://visitor-badge.glitch.me/badge?page_id=linaqruf.lora-dreambooth)

# Kohya LoRA Dreambooth<br><small><small>A Colab Notebook For LoRA Training (Dreambooth Method)

Adapted to Google Colab based on [kohya-ss/sd-script](https://github.com/kohya-ss/sd-scripts)<br>
Adapted to Google Colab by [Linaqruf](https://github.com/Linaqruf)<br>
You can find latest notebook update [here](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb)

| Notebook Name | Description | Link |
| --- | --- | --- |
| [Kohya LoRA Dreambooth](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb) | LoRA Training (Dreambooth method) | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=for-the-badge)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb) |
| [Kohya LoRA Fine-Tuning](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-finetuner.ipynb) | LoRA Training (Fine-tune method) | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=for-the-badge)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-finetuner.ipynb) |
| [Kohya Trainer](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-trainer.ipynb) | Native Training | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=for-the-badge)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-trainer.ipynb) |
| [Kohya Dreambooth](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-dreambooth.ipynb) | Dreambooth Training | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=for-the-badge)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-dreambooth.ipynb) |
| Kohya Textual Inversion  | Textual Inversion Training | SOON |

# I. Install Kohya Trainer
"""

# Commented out IPython magic to ensure Python compatibility.
#@title ## 1.1. Clone Kohya Trainer
#@markdown Clone Kohya Trainer from GitHub and check for updates. Use textbox below if you want to checkout other branch or old commit. Leave it empty to stay the HEAD on main.

import os
# %store -r

!nvidia-smi

root_dir = "/content"
# %store root_dir
repo_dir = str(root_dir)+"/kohya-trainer"
# %store repo_dir
tools_dir = str(root_dir)+"/kohya-trainer/tools"
# %store tools_dir 
finetune_dir = str(root_dir)+"/kohya-trainer/finetune"
# %store finetune_dir
training_dir = str(root_dir)+"/dreambooth"
# %store training_dir

branch = "" #@param {type: "string"}
repo_url = "https://github.com/Linaqruf/kohya-trainer"

def clone_repo():
  if os.path.isdir(repo_dir):
    print("The repository folder already exists, will do a !git pull instead\n")
#     %cd {repo_dir}
    !git pull origin {branch} if branch else !git pull
  else:
#     %cd {root_dir}
    !git clone {repo_url} {repo_dir}

if not os.path.isdir(repo_dir):
  clone_repo()

# %cd {root_dir}
os.makedirs(repo_dir, exist_ok=True)
os.makedirs(tools_dir, exist_ok=True)
os.makedirs(finetune_dir, exist_ok=True)
os.makedirs(training_dir, exist_ok=True)

if branch:
#   %cd {repo_dir}
  status = os.system(f"git checkout {branch}")
  if status != 0:
    raise Exception("Failed to checkout branch or commit")

# Commented out IPython magic to ensure Python compatibility.
#@title ## 1.2. Installing Dependencies
#@markdown This will install required Python packages
import os
# %store -r

# %cd {repo_dir}

accelerate_config = os.path.join(repo_dir, "accelerate_config/config.yaml")
# %store accelerate_config
install_xformers = True #@param {'type':'boolean'}

def install_dependencies():
    !pip -q install --upgrade gallery-dl gdown imjoy-elfinder
    !apt -q install liblz4-tool aria2
    !pip -q install --upgrade -r requirements.txt

    if install_xformers:
        !pip -q install -U -I --no-deps https://github.com/camenduru/stable-diffusion-webui-colab/releases/download/0.0.15/xformers-0.0.15.dev0+189828c.d20221207-cp38-cp38-linux_x86_64.whl

    from accelerate.utils import write_basic_config
    if not os.path.exists(accelerate_config):
        write_basic_config(save_location=accelerate_config)

install_dependencies()

"""## 1.3. Sign-in to Cloud Service"""

# Commented out IPython magic to ensure Python compatibility.
#@title ### 1.3.1. Login to Huggingface hub
from huggingface_hub import login
# %store -r

#@markdown Login to Huggingface hub
#@markdown 1. You need a Huggingface account.
#@markdown 2. To create a huggingface token, go to https://huggingface.co/settings/tokens, then create a new token or copy an available token with the `Write` role.
write_token = "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" #@param {type:"string"}
login(write_token, add_to_git_credential=True)

# %store write_token

#@title ### 1.3.2. Mount Drive (Optional)
from google.colab import drive

mount_drive = True #@param {type: "boolean"}

if mount_drive:
  drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
#@title 1.4. Open Special `File Explorer` for Colab (Optional)
#@markdown This will work in real-time even while you run other cells.
# %store -r

import threading
from google.colab import output
from imjoy_elfinder.app import main

thread = threading.Thread(target=main, args=[["--root-dir=/content", "--port=8765"]])
thread.start()

open_in_new_tab = True #@param {type:"boolean"}

if open_in_new_tab:
  output.serve_kernel_port_as_window(8765)
else:
  output.serve_kernel_port_as_iframe(8765, height='500')

"""# II. Pretrained Model Selection"""

# Commented out IPython magic to ensure Python compatibility.
#@title ## 2.1. Download Available Model 
import os
# %store -r

# %cd {root_dir}

installModels = []
installv2Models = []

#@markdown ### Available Model
#@markdown Select one of available model to download:

#@markdown ### SD1.x model
modelUrl = ["", \
            "https://huggingface.co/Linaqruf/personal-backup/resolve/main/models/animefull-final-pruned.ckpt", \
            "https://huggingface.co/cag/anything-v3-1/resolve/main/anything-v3-1.safetensors", \
            "https://huggingface.co/andite/anything-v4.0/resolve/main/anything-v4.5-pruned.ckpt", \
            "https://huggingface.co/Rasgeath/self_made_sauce/resolve/main/Kani-anime-pruned.ckpt", \
            "https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix2/AbyssOrangeMix2_nsfw.safetensors", \
            "https://huggingface.co/gsdf/Counterfeit-V2.0/resolve/main/Counterfeit-V2.0fp16.safetensors", \
            "https://huggingface.co/closertodeath/dpepteahands3/resolve/main/dpepteahand3.ckpt", \
            "https://huggingface.co/prompthero/openjourney-v2/resolve/main/openjourney-v2.ckpt", \
            "https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/resolve/main/dreamlike-diffusion-1.0.ckpt", \
            "https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt"]
modelList = ["", \
             "Animefull-final-pruned", \
             "Anything-v3-1", \
             "Anything-v4-5-pruned", \
             "Kani-anime-pruned", \
             "AbyssOrangeMix2-nsfw", \
             "Counterfeit-v2", \
             "DpepTeaHands3", \
             "OpenJourney-v2", \
             "Dreamlike-diffusion-v1-0", \
             "Stable-Diffusion-v1-5"]
modelName = "Stable-Diffusion-v1-5"  #@param ["", "Animefull-final-pruned", "Anything-v3-1", "Anything-v4-5-pruned", "Kani-anime-pruned", "AbyssOrangeMix2-nsfw", "Counterfeit-v2", "DpepTeaHands3", "OpenJourney-v2", "Dreamlike-diffusion-v1-0", "Stable-Diffusion-v1-5"]

#@markdown ### SD2.x model
v2ModelUrl = ["", \
              "https://huggingface.co/stabilityai/stable-diffusion-2-1-base/resolve/main/v2-1_512-ema-pruned.ckpt", \
              "https://huggingface.co/stabilityai/stable-diffusion-2-1/resolve/main/v2-1_768-ema-pruned.ckpt", \
              "https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/wd-1-4-anime_e2.ckpt", \
              "https://huggingface.co/p1atdev/pd-archive/resolve/main/plat-v1-3-1.safetensors"]
v2ModelList = ["", \
              "stable-diffusion-2-1-base", \
              "stable-diffusion-2-1-768v", \
              "waifu-diffusion-1-4-anime-e2", \
              "plat-diffusion-v1-3-1"]
v2ModelName = "" #@param ["", "stable-diffusion-2-1-base", "stable-diffusion-2-1-768v", "waifu-diffusion-1-4-anime-e2", "plat-diffusion-v1-3-1"]

if modelName != "":
  installModels.append((modelName, modelUrl[modelList.index(modelName)]))
if v2ModelName != "":
  installv2Models.append((v2ModelName, v2ModelUrl[v2ModelList.index(v2ModelName)]))

def install(checkpoint_name, url):
  ext = "ckpt" if url.endswith(".ckpt") else "safetensors"

  hf_token = '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' 
  user_header = f"\"Authorization: Bearer {hf_token}\""
  !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {root_dir}/pre_trained_model -o {checkpoint_name}.{ext} "{url}"

def install_checkpoint():
  for model in installModels:
    install(model[0], model[1])
  for v2model in installv2Models:
    install(v2model[0], v2model[1])

install_checkpoint()

# Commented out IPython magic to ensure Python compatibility.
#@title ## 2.2. Download Custom Model

import os
# %store -r

# %cd {root_dir}

#@markdown ### Custom model
modelUrl = "" #@param {'type': 'string'}
dst = str(root_dir)+"/pre_trained_model"

if not os.path.exists(dst):
    os.makedirs(dst)

def install(url):
  base_name = os.path.basename(url)

  if url.startswith("https://drive.google.com"):
#     %cd {dst}
    !gdown --fuzzy {url}
  elif url.startswith("https://huggingface.co/"):
    if '/blob/' in url:
      url = url.replace('/blob/', '/resolve/')
    #@markdown Change this part with your own huggingface token if you need to download your private model
    hf_token = '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' #@param {type:"string"}
    user_header = f"\"Authorization: Bearer {hf_token}\""
    !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {dst} -o {base_name} {url}
  else:
    !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d {dst} -o {url}

install(modelUrl)

# Commented out IPython magic to ensure Python compatibility.
#@title ## 2.3. Download Available VAE (Optional)
# %store -r 

# %cd {root_dir}

installVae = []
#@markdown ### Available VAE
#@markdown Select one of the VAEs to download, select `none` for not download VAE:
vaeUrl = ["", \
          "https://huggingface.co/Linaqruf/personal-backup/resolve/main/vae/animevae.pt", \
          "https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime.ckpt", \
          "https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt"]
vaeList = ["none", \
           "anime.vae.pt", \
           "waifudiffusion.vae.pt", \
           "stablediffusion.vae.pt"]
vaeName = "anime.vae.pt" #@param ["none", "anime.vae.pt", "waifudiffusion.vae.pt", "stablediffusion.vae.pt"]

installVae.append((vaeName, vaeUrl[vaeList.index(vaeName)]))

def install(vae_name, url):
  hf_token = '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'
  user_header = f"\"Authorization: Bearer {hf_token}\""
  !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -o vae/{vae_name} "{url}"

def install_vae():
  if vaeName != "none":
    for vae in installVae:
      install(vae[0], vae[1])
  else:
    pass

install_vae()

"""# III. Data Acquisition

You have two options for acquiring your dataset: uploading it to this notebook or bulk downloading images from Danbooru using the image scraper. If you prefer to use your own dataset, simply upload it to Colab's local files.

"""

# Commented out IPython magic to ensure Python compatibility.
#@title ## 3.1. Define Train Data Directory
#@markdown Define the location of your training data. This cell will also create a folder based on your input. Leave `reg_data_dir` empty for now.
# %store -r

parent_directory = "/content/drive/MyDrive/loratrain/image" #@param {type: "string"}
# %store parent_directory
reg_folder_directory = os.path.join(os.path.dirname(parent_directory), "reg_data")
# %store reg_folder_directory

reg_repeats = 1 #@param {type: "integer"}
train_repeats = 10 #@param {type: "integer"}
concept_name = "jiangshuying" #@param {type: "string"}
class_name = "1girl" #@param {type: "string"}
#@markdown You can run this cell multiple time to add new concepts

def get_folder_name(repeats, class_name, concept_name=None):
  if class_name:
    return f"{repeats}_{concept_name} {class_name}" if concept_name else f"{repeats}_{class_name}"
  return f"{repeats}_{concept_name}"

train_folder = get_folder_name(train_repeats, class_name, concept_name=concept_name)
reg_folder = get_folder_name(reg_repeats, class_name)

train_data_dir = os.path.join(parent_directory, train_folder)
reg_data_dir = os.path.join(reg_folder_directory, reg_folder)

os.makedirs(parent_directory, exist_ok=True)
os.makedirs(reg_folder_directory, exist_ok=True)
os.makedirs(train_data_dir, exist_ok=True)
os.makedirs(reg_data_dir, exist_ok=True)

# Commented out IPython magic to ensure Python compatibility.
#@title ## 3.2. Download and Extract Zip (.zip)
import os
import shutil
from pathlib import Path
# %store -r

#@markdown ### Define Zipfile URL or Zipfile Path
zipfile_url_or_path = "https://huggingface.co/datasets/Linaqruf/your-dataset-name/resolve/main/masabodo_dataset.zip" #@param {'type': 'string'}
zipfile_dst = str(root_dir)+"/zip_file.zip"
extract_to = "" #@param {'type': 'string'}

if extract_to != "":
  os.makedirs(extract_to, exist_ok=True)
else:
  extract_to = train_data_dir

#@markdown This will ignore `extract_to` path and automatically extracting to `train_data_dir`
is_dataset = True #@param{'type':'boolean'}

#@markdown Tick this if you want to extract all files directly to `extract_to` folder, and automatically delete the zip to save the memory
auto_unzip_and_delete = True #@param{'type':'boolean'}

dirname = os.path.dirname(zipfile_dst)
basename = os.path.basename(zipfile_dst)

try:
  if zipfile_url_or_path.startswith("/content"):
    zipfile_dst = zipfile_url_or_path
    if auto_unzip_and_delete == False:
      if is_dataset:
        extract_to = train_data_dir
      !unzip -j {zipfile_dst} -d "{extract_to}"
  elif zipfile_url_or_path.startswith("https://drive.google.com"):
    !gdown --fuzzy  {zipfile_url_or_path}
  elif zipfile_url_or_path.startswith("magnet:?"):
    !aria2c --summary-interval=10 -c -x 10 -k 1M -s 10 {zipfile_url_or_path}
  elif zipfile_url_or_path.startswith("https://huggingface.co/"):
    if '/blob/' in zipfile_url_or_path:
      zipfile_url_or_path = zipfile_url_or_path.replace('/blob/', '/resolve/')

    hf_token = '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'
    user_header = f"\"Authorization: Bearer {hf_token}\""
    !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {dirname} -o {basename} {zipfile_url_or_path}
  else:
    !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d {dirname} -o {basename} {zipfile_url_or_path}

except Exception as e:
  print("An error occurred while downloading the file:", e)

if is_dataset:
  extract_to = train_data_dir

if auto_unzip_and_delete:
  !unzip -j {zipfile_dst} -d "{extract_to}"

  files_to_move = ("meta_cap.json", \
                   "meta_cap_dd.json", \
                   "meta_lat.json", \
                   "meta_clean.json")

  for filename in os.listdir(extract_to):
      file_path = os.path.join(extract_to, filename)
      if filename in files_to_move:
          shutil.move(file_path, os.path.dirname(extract_to))
  
  path_obj = Path(zipfile_dst)
  zipfile_name = path_obj.parts[-1]
  
  if os.path.isdir(zipfile_dst):
    print("\nThis zipfile doesn't exist or has been deleted \n")
  else:
    os.remove(zipfile_dst)
    print(f"\n{zipfile_name} has been deleted")

# Commented out IPython magic to ensure Python compatibility.
#@title ## 3.3. Simple Booru Scraper (Optional)
#@markdown Use gallery-dl to scrape images from a booru site using the specified tags
import os
import html
# %store -r

# %cd {root_dir}

booru = "Gelbooru" #@param ["", "Danbooru", "Gelbooru"]
tag1 = "cleo_(dragalia_lost) " #@param {type: "string"}
tag2 = "" #@param {type: "string"}
download_tags = False #@param {type: "boolean"}

if tag2 != "":
  tags = tag1 + "+" + tag2
else:
  tags = tag1

write_tags = "--write-tags" if download_tags == True else ""

if booru.lower() == "danbooru":
  !gallery-dl "https://danbooru.donmai.us/posts?tags={tags}" {write_tags} -D "{train_data_dir}"
elif booru.lower() == "gelbooru":
  !gallery-dl "https://gelbooru.com/index.php?page=post&s=list&tags={tags}" {write_tags} -D "{train_data_dir}"
else:
  print(f"Unknown booru site: {booru}")

if download_tags == True: 
  files = [f for f in os.listdir(train_data_dir) if f.endswith(".txt")]

  for file in files:
      file_path = os.path.join(train_data_dir, file)

      with open(file_path, "r") as f:
          contents = f.read()

      contents = html.unescape(contents)
      contents = contents.replace("_", " ")
      contents = ", ".join(contents.split("\n"))

      with open(file_path, "w") as f:
          f.write(contents)

"""# IV. Data Preprocessing"""

#@title ## 4.1. RGBA to RGB converter (Optional)

import random
from tqdm import tqdm
import concurrent.futures
from PIL import Image

#@markdown Transparent images can negatively impact training results, resulting in good characters with bad backgrounds after the model is finished. To address this issue, this code will convert your transparent dataset with alpha channel (RGBA) to RGB and give it a white background. Alternatively, you can set random colors for a more diverse dataset. 

#@markdown `NOTE: All images endswith .webp will be deleted after the conversion process is completed.`

random_color = False #@param {type:"boolean"}

batch_size = 32 #@param {type:"number"}

images = [image for image in os.listdir(train_data_dir) if image.endswith('.png') or image.endswith('.webp')]
background_colors = [(255, 255, 255), 
                     (0, 0, 0), 
                     (255, 0, 0), 
                     (0, 255, 0), 
                     (0, 0, 255), 
                     (255, 255, 0), 
                     (255, 0, 255), 
                     (0, 255, 255)]

def process_image(image_name):
    img = Image.open(f'{train_data_dir}/{image_name}')

    if img.mode in ('RGBA', 'LA'):
        if random_color:
          background_color = random.choice(background_colors)
        else:
          background_color = (255, 255, 255)
        bg = Image.new('RGB', img.size, background_color)
        bg.paste(img, mask=img.split()[-1])

        if image_name.endswith('.webp'):
            bg = bg.convert('RGB')
            bg.save(f'{train_data_dir}/{image_name.replace(".webp", ".jpg")}', "JPEG")
            os.remove(f'{train_data_dir}/{image_name}')
            print(f" Converted image: {image_name} to {image_name.replace('.webp', '.jpg')}")
        else:
            bg.save(f'{train_data_dir}/{image_name}', "PNG")
            print(f" Converted image: {image_name}")
    else:
        if image_name.endswith('.webp'):
            img.save(f'{train_data_dir}/{image_name.replace(".webp", ".jpg")}', "JPEG")
            os.remove(f'{train_data_dir}/{image_name}')
            print(f" Converted image: {image_name} to {image_name.replace('.webp', '.jpg')}")
        else:
            img.save(f'{train_data_dir}/{image_name}', "PNG")

num_batches = len(images) // batch_size + 1

with concurrent.futures.ThreadPoolExecutor() as executor:
    for i in tqdm(range(num_batches)):
        start = i * batch_size
        end = start + batch_size
        batch = images[start:end]
        executor.map(process_image, batch)

print("All images have been converted")

# Commented out IPython magic to ensure Python compatibility.
#@title ## 4.2. Image Upscaler (Optional)
from IPython.utils import capture
# %cd {root_dir}

#@markdown Useful if your dataset has image with resolution lower than `512x512`, this will take some time and consume more gpu power.
upscaler_model = "RealESRGAN_x4plus_anime_6B" #@param ['RealESRGAN_x4plus','RealESRNet_x4plus','RealESRGAN_x4plus_anime_6B','RealESRGAN_x2plus','realesr-animevideov3','realesr-general-x4v3']
upcale_by = 2.5 #@param {type:"slider", min:1, max:5, step:0.1}
face_enhance = False #@param {type:"boolean"}

if not os.path.exists(f"{root_dir}/Real-ESRGAN"):
  print("Installing dependencies, please wait...")
  with capture.capture_output() as cap:
    !git clone https://github.com/xinntao/Real-ESRGAN.git
#     %cd {root_dir}/Real-ESRGAN
    !pip -q install basicsr
    !pip -q install facexlib
    !pip -q install gfpgan
    !pip -q install -r requirements.txt
    !python setup.py develop
    del cap

# %cd {root_dir}/Real-ESRGAN
!python inference_realesrgan.py \
  -n "{upscaler_model}" \
  -i "{train_data_dir}" \
  -o "{train_data_dir}" \
  --suffix "" \
  --outscale {upcale_by} \
  {"--face_enhance" if face_enhance else ""}

# Commented out IPython magic to ensure Python compatibility.
#@title ## 4.3. Data Cleaning
#@markdown This will delete unnecessary files and unsupported media like `.mp4`, `.webm`, and `.gif`
# %store -r

import os

# %cd {root_dir}

test = os.listdir(train_data_dir)

#@markdown I recommend to `keep_metadata` especially if you're doing resume training and you have metadata and bucket latents file from previous training like `.npz`, `.txt`, `.caption`, and `json`.
keep_metadata = True #@param {'type':'boolean'}

if keep_metadata == True:
  supported_types = [".png", ".jpg", ".jpeg", ".bmp", ".webp", ".caption", ".npz", ".txt", ".json"]
else:
  supported_types = [".png", ".jpg", ".jpeg", ".bmp", ".webp"]

for item in test:
    file_ext = os.path.splitext(item)[1]
    if file_ext not in supported_types:
        print(f"Deleting file {item} from {train_data_dir}")
        os.remove(os.path.join(train_data_dir, item))

"""## 4.4. Data Annotation (Optional)
You can choose to train a model using caption. We're using [GIT: GenerativeImage2Text](https://huggingface.co/models?search=microsoft/git) for image captioning and [Waifu Diffusion 1.4 Tagger](https://huggingface.co/spaces/SmilingWolf/wd-v1-4-tags) for image tagging like danbooru.
- Use GIT Captioning for: `Images in General`
- Use Waifu Diffusion 1.4 Tagger V2 for: `Anime and manga-styled Images`
"""

# Commented out IPython magic to ensure Python compatibility.
#@title ### 4.4.1. GIT Captioning
import glob
# %store -r

# %cd {finetune_dir}

#@markdown [GIT: GenerativeImage2Text](https://huggingface.co/models?search=microsoft/git) is an image-to-text model published by [Microsoft](https://github.com/microsoft/GenerativeImage2Text). GIT is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using "teacher forcing" on a lot of (image, text) pairs. 
#@markdown Example: `astronaut riding a horse in space`. 

batch_size = 8 #@param {type:'number'}
max_data_loader_n_workers = 2 #@param {type:'number'}
model = "microsoft/git-large-textcaps" #@param ["microsoft/git-base", "microsoft/git-large-coco", "microsoft/git-base-coco", "microsoft/git-base-vatex", "microsoft/git-base-textcaps", "microsoft/git-large-vatex", "microsoft/git-base-textvqa", "microsoft/git-large-textcaps", "microsoft/git-large-r", "microsoft/git-base-vqav2", "microsoft/git-large-r-textcaps", "microsoft/git-large", "microsoft/git-large-vqav2", "microsoft/git-large-textvqa", "microsoft/git-base-msrvtt-qa", "microsoft/git-large-msrvtt-qa", "microsoft/git-large-r-coco"]
max_length = 50 #@param {type:"slider", min:0, max:100, step:1.0}
undesired_words = "" #@param {type:'string'}

!python make_captions_by_git.py \
  "{train_data_dir}" \
  --model_id {model} \
  --batch_size {batch_size} \
  --max_length {max_length} \
  --remove_words {undesired_words} \
  --caption_extension .caption \
  --max_data_loader_n_workers {max_data_loader_n_workers}

# Commented out IPython magic to ensure Python compatibility.
#@title ### 4.4.2. Waifu Diffusion 1.4 Tagger V2
import glob
# %store -r

# %cd {finetune_dir}

#@markdown [Waifu Diffusion 1.4 Tagger V2](https://huggingface.co/spaces/SmilingWolf/wd-v1-4-tags) is Danbooru-styled image classification developed by [SmilingWolf](https://github.com/SmilingWolf). It's intended to classify anime and manga-like artwork but can be useful for general use.
#@markdown Example: `1girl, solo, looking_at_viewer, short_hair, bangs, simple_background`. 
batch_size = 8 #@param {type:'number'}
max_data_loader_n_workers = 2 #@param {type:'number'}
model = "Swin_V2" #@param ["Swin_V2", "Convnext_V2", "ViT_V2"]
#@markdown It's recommended to set threshold higher (i.e. `0.85`) if you train on object or character, and lower the threshold (i.e `0.35`) to train on general, style or environment.
threshold = 0.35 #@param {type:"slider", min:0, max:1, step:0.05}

if model == "Swin_V2":
  repo_id = "SmilingWolf/wd-v1-4-swinv2-tagger-v2"
elif model == "Convnext_V2":
  repo_id = "SmilingWolf/wd-v1-4-convnext-tagger-v2"
else:
  repo_id = "SmilingWolf/wd-v1-4-vit-tagger-v2"

!python tag_images_by_wd14_tagger.py \
  "{train_data_dir}" \
  --batch_size {batch_size} \
  --repo_id {repo_id} \
  --thresh {threshold} \
  --caption_extension .txt \
  --max_data_loader_n_workers {max_data_loader_n_workers}

"""# V. Training Model


"""

# Commented out IPython magic to ensure Python compatibility.
#@title ## 5.1. Define Important folder
from google.colab import drive
# %store -r

v2 = False #@param {type:"boolean"}
v_parameterization = False #@param {type:"boolean"}
project_name = "jiangshuying" #@param {type:"string"}
pretrained_model_name_or_path = "/content/pre_trained_model/Stable-Diffusion-v1-5.ckpt" #@param {type:"string"}
vae = ""  #@param {type:"string"}
#@markdown You need to register parent folder and not where `train_data_dir` located
train_folder_directory = "/content/drive/MyDrive/loratrain/image" #@param {'type':'string'}
# %store train_folder_directory
reg_folder_directory = "/content/drive/MyDrive/loratrain/image" #@param {'type':'string'}
# %store reg_folder_directory
output_dir = "/content/drive/MyDrive/loratrain/" #@param {'type':'string'}
resume_path =""
inference_url = "https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/configs/stable-diffusion/"

#@markdown This will ignore `output_dir` defined above, and changed to `/content/drive/MyDrive/fine_tune/output` by default
output_to_drive = True #@param {'type':'boolean'}

if output_to_drive:
  output_dir = "/content/drive/MyDrive/dreambooth/output"

  if not os.path.exists("/content/drive"):
    drive.mount('/content/drive')  

os.makedirs(output_dir, exist_ok=True)

if v2 and not v_parameterization:
  inference_url += "v2-inference.yaml"
if v2 and v_parameterization:
  inference_url += "v2-inference-v.yaml"

try:
  if v2:
    !wget {inference_url} -O {output_dir}/{project_name}.yaml
    print("File successfully downloaded")
except:
  print("There was an error downloading the file. Please check the URL and try again.")

# Commented out IPython magic to ensure Python compatibility.
#@title ## 5.2. Define Specific LoRA Training Parameters
# %store -r

#@markdown ## LoRA - Low Rank Adaptation Dreambooth

#@markdown Some people recommend setting the `network_dim` to a higher value.
network_dim = 128 #@param {'type':'number'}
#@markdown For weight scaling in LoRA, it is better to set `network_alpha` the same as `network_dim` unless you know what you're doing. A lower `network_alpha` requires a higher learning rate. For example, if `network_alpha = 1`, then `unet_lr = 1e-3`.
network_alpha = 128 #@param {'type':'number'}
network_module = "networks.lora"

#@markdown `network_weights` can be specified to resume training.
network_weights = "" #@param {'type':'string'}

#@markdown By default, both Text Encoder and U-Net LoRA modules are enabled. Use `network_train_on` to specify which module to train.
network_train_on = "both" #@param ['both','unet_only', 'text_encoder_only'] {'type':'string'}

#@markdown It is recommended to set the `text_encoder_lr` to a lower learning rate, such as `5e-5`, or to set `text_encoder_lr = 1/2 * unet_lr`.
learning_rate = 1e-4 #@param {'type':'number'}
unet_lr = 0 #@param {'type':'number'}
text_encoder_lr = 5e-5 #@param {'type':'number'}
lr_scheduler = "constant" #@param ["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"] {allow-input: false}

#@markdown If `lr_scheduler = cosine_with_restarts`, update `lr_scheduler_num_cycles`.
lr_scheduler_num_cycles = 1 #@param {'type':'number'}
#@markdown If `lr_scheduler = polynomial`, update `lr_scheduler_power`.
lr_scheduler_power = 1 #@param {'type':'number'}

#@markdown Check the box to not save metadata in the output model.
no_metadata = False #@param {type:"boolean"}
training_comment = "this comment will be stored in the metadata" #@param {'type':'string'}

print("Loading network module:", network_module)
print(f"{network_module} dim set to:", network_dim)
print(f"{network_module} alpha set to:", network_alpha)

if network_weights == "":
  print("No LoRA weight loaded.")
else:
  if os.path.exists(network_weights):
    print("Loading LoRA weight:", network_weights)
  else:
    print(f"{network_weights} does not exist.")
    network_weights =""

if network_train_on == "unet_only":
  print("Enabling LoRA for U-Net.")
  print("Disabling LoRA for Text Encoder.")

print("Global learning rate: ", learning_rate)

if network_train_on == "unet_only":
  print("Enable LoRA for U-Net")
  print("Disable LoRA for Text Encoder")
  print("UNet learning rate: ", unet_lr) if unet_lr != 0 else ""
if network_train_on == "text_encoder_only":
  print("Disabling LoRA for U-Net")
  print("Enabling LoRA for Text Encoder")
  print("Text encoder learning rate: ", text_encoder_lr) if text_encoder_lr != 0 else ""
else:
  print("Enabling LoRA for U-Net")
  print("Enabling LoRA for Text Encoder")
  print("UNet learning rate: ", unet_lr) if unet_lr != 0 else ""
  print("Text encoder learning rate: ", text_encoder_lr) if text_encoder_lr != 0 else ""

print("Learning rate Scheduler:", lr_scheduler)

if lr_scheduler == "cosine_with_restarts":
  print("- Number of cycles: ", lr_scheduler_num_cycles)
elif lr_scheduler == "polynomial":
  print("- Power: ", lr_scheduler_power)

# Printing the training comment if metadata is not disabled and a comment is present
if not no_metadata:
  if training_comment: 
    training_comment = training_comment.replace(" ", "_")
    print("Training comment:", training_comment)
else:
  print("Metadata won't be saved")

# Commented out IPython magic to ensure Python compatibility.
from prettytable import PrettyTable
import textwrap
import yaml

# %store -r

#@title ## 5.3. Start LoRA Dreambooth
#@markdown ### Define Parameter

train_batch_size = 6 #@param {type:"number"}
num_epochs = 20 #@param {type:"number"}
caption_extension = '.txt' #@param {'type':'string'}
mixed_precision = "fp16" #@param ["no","fp16","bf16"] {allow-input: false}
save_precision = "fp16" #@param ["float", "fp16", "bf16"] {allow-input: false}
save_n_epochs_type = "save_n_epoch_ratio" #@param ["save_every_n_epochs", "save_n_epoch_ratio"] {allow-input: false}
save_n_epochs_type_value = 3 #@param {type:"number"}
save_model_as = "safetensors" #@param ["ckpt", "pt", "safetensors"] {allow-input: false}
resolution = 512 #@param {type:"slider", min:512, max:1024, step:128}
enable_bucket = True #@param {type:"boolean"}
min_bucket_reso = 320 if resolution > 640 else 256
max_bucket_reso = 1280 if resolution > 640 else 1024
cache_latents = True #@param {type:"boolean"}
max_token_length = 225 #@param {type:"number"}
clip_skip = 2 #@param {type:"number"}
use_8bit_adam = True #@param {type:"boolean"}
gradient_checkpointing = False #@param {type:"boolean"}
gradient_accumulation_steps = 1 #@param {type:"number"}
seed = 0 #@param {type:"number"}
logging_dir = "/content/dreambooth/logs"
log_prefix = project_name
additional_argument = "--shuffle_caption --xformers" #@param {type:"string"}
print_hyperparameter = True #@param {type:"boolean"}
prior_loss_weight = 1.0
# %cd {repo_dir}

train_command=f"""
accelerate launch --config_file={accelerate_config} --num_cpu_threads_per_process=8 train_network.py \
  {"--v2" if v2 else ""} \
  {"--v_parameterization" if v2 and v_parameterization else ""} \
  --network_dim={network_dim} \
  --network_alpha={network_alpha} \
  --network_module={network_module} \
  {"--network_weights=" + network_weights if network_weights else ""} \
  {"--network_train_unet_only" if network_train_on == "unet_only" else ""} \
  {"--network_train_text_encoder_only" if network_train_on == "text_encoder_only" else ""} \
  --learning_rate={learning_rate} \
  {"--unet_lr=" + format(unet_lr) if unet_lr !=0 else ""} \
  {"--text_encoder_lr=" + format(text_encoder_lr) if text_encoder_lr !=0 else ""} \
  {"--no_metadata" if no_metadata else ""} \
  {"--training_comment=" + training_comment if training_comment and not no_metadata else ""} \
  --lr_scheduler={lr_scheduler} \
  {"--lr_scheduler_num_cycles=" + format(lr_scheduler_num_cycles) if lr_scheduler == "cosine_with_restarts" else ""} \
  {"--lr_scheduler_power=" + format(lr_scheduler_power) if lr_scheduler == "polynomial" else ""} \
  --pretrained_model_name_or_path={pretrained_model_name_or_path} \
  {"--vae=" + vae if vae else ""} \
  {"--caption_extension=" + caption_extension if caption_extension else ""} \
  --train_data_dir={train_folder_directory} \
  --reg_data_dir={reg_folder_directory} \
  --output_dir={output_dir} \
  --prior_loss_weight={prior_loss_weight} \
  {"--resume=" + resume_path if resume_path else ""} \
  {"--output_name=" + project_name if project_name else ""} \
  --mixed_precision={mixed_precision} \
  --save_precision={save_precision} \
  {"--save_every_n_epochs=" + format(save_n_epochs_type_value) if save_n_epochs_type=="save_every_n_epochs" else ""} \
  {"--save_n_epoch_ratio=" + format(save_n_epochs_type_value) if save_n_epochs_type=="save_n_epoch_ratio" else ""} \
  --save_model_as={save_model_as} \
  --resolution={resolution} \
  {"--enable_bucket" if enable_bucket else ""} \
  {"--min_bucket_reso=" + format(min_bucket_reso) if enable_bucket else ""} \
  {"--max_bucket_reso=" + format(max_bucket_reso) if enable_bucket else ""} \
  {"--cache_latents" if cache_latents else ""} \
  --train_batch_size={train_batch_size} \
  --max_token_length={max_token_length} \
  {"--use_8bit_adam" if use_8bit_adam else ""} \
  --max_train_epochs={num_epochs} \
  {"--seed=" + format(seed) if seed > 0 else ""} \
  {"--gradient_checkpointing" if gradient_checkpointing else ""} \
  {"--gradient_accumulation_steps=" + format(gradient_accumulation_steps) } \
  {"--clip_skip=" + format(clip_skip) if v2 == False else ""} \
  --logging_dir={logging_dir} \
  --log_prefix={log_prefix} \
  {additional_argument}
  """

debug_params = ["v2", \
                "v_parameterization", \
                "network_dim", \
                "network_alpha", \
                "network_module", \
                "network_weights", \
                "network_train_on", \
                "learning_rate", \
                "unet_lr", \
                "text_encoder_lr", \
                "no_metadata", \
                "training_comment", \
                "lr_scheduler", \
                "lr_scheduler_num_cycles", \
                "lr_scheduler_power", \
                "pretrained_model_name_or_path", \
                "vae", \
                "caption_extension", \
                "train_folder_directory", \
                "reg_folder_directory", \
                "output_dir", \
                "prior_loss_weight", \
                "resume_path", \
                "project_name", \
                "mixed_precision", \
                "save_precision", \
                "save_n_epochs_type", \
                "save_n_epochs_type_value", \
                "save_model_as", \
                "resolution", \
                "enable_bucket", \
                "min_bucket_reso", \
                "max_bucket_reso", \
                "cache_latents", \
                "train_batch_size", \
                "max_token_length", \
                "use_8bit_adam", \
                "num_epochs", \
                "seed", \
                "gradient_checkpointing", \
                "gradient_accumulation_steps", \
                "clip_skip", \
                "logging_dir", \
                "log_prefix", \
                "additional_argument"]

if print_hyperparameter:
    table = PrettyTable()
    table.field_names = ["Hyperparameter", "Value"]
    for params in debug_params:
        if params != "":
            if globals()[params] == "":
                value = "False"
            else:
                value = globals()[params]
            table.add_row([params, value])
    table.align = "l"
    print(table)

    arg_list = train_command.split()
    mod_train_command = {'command': arg_list}
    
    train_folder = os.path.dirname(output_dir)
    
    # save the YAML string to a file
    with open(str(train_folder)+'/dreambooth_lora_cmd.yaml', 'w') as f:
        yaml.dump(mod_train_command, f)

f = open("./train.sh", "w")
f.write(train_command)
f.close()
!chmod +x ./train.sh
!./train.sh

"""# VI. Testing"""

!ls "/content/drive/MyDrive/dreambooth/output"

#@title ## 6.1. Validating LoRA Weights

#@markdown Now you can check if your LoRA trained properly. 

#@markdown  If you used `clip_skip = 2` during training, the values of `lora_te_text_model_encoder_layers_11_*` will be `0.0`, this is normal. These layers are not trained at this value of `Clip Skip`.
network_weights = "/content/dreambooth/output/neurosama.safetensors" #@param {'type':'string'}
no_verbose = True #@param {type:"boolean"}

import os
import torch
import json
from safetensors.torch import load_file
from safetensors.torch import safe_open
import library.model_util as model_util

print("Loading LoRA weight:", network_weights)

def main(file, verbose:bool):
  if not verbose:
    sd = load_file(file) if os.path.splitext(file)[1] == '.safetensors' else torch.load(file, map_location='cuda')
    values = []

    keys = list(sd.keys())
    for key in keys:
      if 'lora_up' in key or 'lora_down' in key:
        values.append((key, sd[key]))
    print(f"number of LoRA modules: {len(values)}")

    for key, value in values:
      value = value.to(torch.float32)
      print(f"{key},{torch.mean(torch.abs(value))},{torch.min(torch.abs(value))}")
  
  if model_util.is_safetensors(file):
      with safe_open(file, framework="pt") as f:
          metadata = f.metadata()
      if metadata is not None:
        if not verbose:
          metadata["ss_bucket_info"] = json.loads(metadata["ss_bucket_info"].replace("\\", ""))
          metadata["ss_tag_frequency"] = json.loads(metadata["ss_tag_frequency"].replace("\\", ""))
        metadata["ss_dataset_dirs"] = json.loads(metadata["ss_dataset_dirs"].replace("\\", ""))
        metadata["ss_reg_dataset_dirs"] = json.loads(metadata["ss_reg_dataset_dirs"].replace("\\", ""))
        print(f"\nLoad metadata for: {file}")
        print(json.dumps(metadata, indent=4))
  else:
      print("No metadata saved, your model is not in safetensors format")

if __name__ == '__main__':
  main(network_weights, no_verbose)

# Commented out IPython magic to ensure Python compatibility.
#@title ## 6.2. Inference
# %store -r

#@markdown LoRA Config
network_weights = "/content/dreambooth/output/neurosama.safetensors" #@param {'type':'string'}
network_module = "networks.lora"
network_mul = 0.6 #@param {'type':'number'}

#@markdown Other Config
v2 = False #@param {type:"boolean"}
v_parameterization = False #@param {type:"boolean"}
instance_prompt = "" #@param {type: "string"}
prompt = "masterpiece, best quality, 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt" #@param {type: "string"}
negative = "lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry" #@param {type: "string"}
model = "/content/pre_trained_model/Anything-v3-1.safetensors" #@param {type: "string"}
vae = "" #@param {type: "string"}
outdir = "/content/tmp" #@param {type: "string"}
scale = 5 #@param {type: "slider", min: 1, max: 40}
sampler = "ddim" #@param ["ddim", "pndm", "lms", "euler", "euler_a", "heun", "dpm_2", "dpm_2_a", "dpmsolver","dpmsolver++", "dpmsingle", "k_lms", "k_euler", "k_euler_a", "k_dpm_2", "k_dpm_2_a"]
steps = 28 #@param {type: "slider", min: 1, max: 100}
precision = "fp16" #@param ["fp16", "bf16"] {allow-input: false}
width = 512 #@param {type: "integer"}
height = 768 #@param {type: "integer"}
images_per_prompt = 4 #@param {type: "integer"}
batch_size = 4 #@param {type: "integer"}
clip_skip = 2 #@param {type: "slider", min: 1, max: 40}
seed = -1 #@param {type: "integer"}

final_prompt = f"{instance_prompt}, {prompt} --n {negative}" if instance_prompt else f"{prompt} --n {negative}"

# %cd {repo_dir}

!python gen_img_diffusers.py \
  {"--v2" if v2 else ""} \
  {"--v_parameterization" if v2 and v_parameterization else ""} \
  --network_module={network_module} \
  --network_weight={network_weights} \
  --network_mul={network_mul} \
  --ckpt={model} \
  --outdir={outdir} \
  --xformers \
  {"--vae=" + vae if vae else ""} \
  --{precision} \
  --W={width} \
  --H={height} \
  {"--seed=" + format(seed) if seed > 0 else ""} \
  --scale={scale} \
  --sampler={sampler} \
  --steps={steps} \
  --max_embeddings_multiples=3 \
  --batch_size={batch_size} \
  --images_per_prompt={images_per_prompt} \
  {"--clip_skip=" + format(clip_skip) if v2 == False else ""} \
  --prompt="{final_prompt}"

# Commented out IPython magic to ensure Python compatibility.
#@title ## 6.3. Visualize loss graph (Optional)
training_logs_path = "/content/dreambooth/logs" #@param {type : "string"}

# %cd /content/kohya-trainer
# %load_ext tensorboard
# %tensorboard --logdir {training_logs_path}

"""# VII. Extras"""

#@title ## 7.1. Compressing model or dataset
import os
import zipfile
import shutil

zip_module = "zipfile" #@param ["zipfile", "shutil", "pyminizip", "zip"]
directory_to_zip = '/content/dreambooth/output' #@param {type: "string"}
output_filename = '/content/neurosama.zip' #@param {type: "string"}
password = "" #@param {type: "string"}

if zip_module == "zipfile":
    with zipfile.ZipFile(output_filename, 'w') as zip:
        for directory_to_zip, dirs, files in os.walk(directory_to_zip):
            for file in files:
                zip.write(os.path.join(directory_to_zip, file))
elif zip_module == "shutil":
    shutil.make_archive(output_filename, 'zip', directory_to_zip)
elif zip_module == "pyminizip":
    !pip install pyminizip
    import pyminizip
    for root, dirs, files in os.walk(directory_to_zip):
        for file in files:
            pyminizip.compress(os.path.join(root, file), "", os.path.join("*",output_filename), password, 5)
elif zip_module == "zip":
    !zip -rv -q -j {output_filename} {directory_to_zip}

"""# VIII. Deployment"""

# Commented out IPython magic to ensure Python compatibility.
#@title ## 8.1. Define your Huggingface Repo

from huggingface_hub import HfApi
from huggingface_hub.utils import validate_repo_id, HfHubHTTPError
# %store -r

api = HfApi()
user = api.whoami(write_token)

#@markdown #### Fill this if you want to upload to your organization, or just leave it empty.

orgs_name = "" #@param{type:"string"}

#@markdown #### If your model/dataset repo didn't exist, it will automatically create your repo.
model_name = "your-model-name" #@param{type:"string"}
dataset_name = "your-dataset-name" #@param{type:"string"}
make_this_model_private = True #@param{type:"boolean"}

if orgs_name == "":
  model_repo = user['name']+"/"+model_name.strip()
  datasets_repo = user['name']+"/"+dataset_name.strip()
else:
  model_repo = orgs_name+"/"+model_name.strip()
  datasets_repo = orgs_name+"/"+dataset_name.strip()

if model_name != "":
  try:
      validate_repo_id(model_repo)
      api.create_repo(repo_id=model_repo, 
                      private=make_this_model_private)
      print("Model Repo didn't exists, creating repo")
      print("Model Repo: ",model_repo,"created!\n")

  except HfHubHTTPError as e:
      print(f"Model Repo: {model_repo} exists, skipping create repo\n")

if dataset_name != "":
  try:
      validate_repo_id(datasets_repo)
      api.create_repo(repo_id=datasets_repo,
                      repo_type="dataset",
                      private=make_this_model_private)
      print("Dataset Repo didn't exists, creating repo")
      print("Dataset Repo",datasets_repo,"created!\n")

  except HfHubHTTPError as e:
      print(f"Dataset repo: {datasets_repo} exists, skipping create repo\n")

"""## 8.2. Upload with `hf_hub`"""

# Commented out IPython magic to ensure Python compatibility.
#@title ### 8.2.1. Upload LoRA
from huggingface_hub import HfApi
from pathlib import Path
# %store -r

api = HfApi()

#@markdown #### This will be uploaded to model repo
model_path = "/content/dreambooth/output" #@param {type :"string"}
path_in_repo = "neurosama_v2" #@param {type :"string"}

#@markdown #### Other Information
commit_message = "" #@param {type :"string"}

if not commit_message:
  commit_message = "feat: upload "+project_name+" lora model"
def upload_model(model_paths, is_folder :bool):
  path_obj = Path(model_paths)
  trained_model = path_obj.parts[-1]
  
  if path_in_repo:
    trained_model = path_in_repo
    
  if is_folder == True:
    print(f"Uploading {trained_model} to https://huggingface.co/"+model_repo)
    print(f"Please wait...")
    
    api.upload_folder(
        folder_path=model_paths,
        path_in_repo=trained_model,
        repo_id=model_repo,
        commit_message=commit_message,
        ignore_patterns=".ipynb_checkpoints"
        )
    print(f"Upload success, located at https://huggingface.co/"+model_repo+"/tree/main\n")
  else: 
    print(f"Uploading {trained_model} to https://huggingface.co/"+model_repo)
    print(f"Please wait...")
            
    api.upload_file(
        path_or_fileobj=model_paths,
        path_in_repo=trained_model,
        repo_id=model_repo,
        commit_message=commit_message,
        )
        
    print(f"Upload success, located at https://huggingface.co/"+model_repo+"/blob/main/"+trained_model+"\n")
      
def upload():
    if model_path.endswith((".ckpt", ".safetensors", ".pt")):
      upload_model(model_path, False)
    else:
      upload_model(model_path, True)

upload()

# Commented out IPython magic to ensure Python compatibility.
#@title ### 8.2.2. Upload Dataset
from huggingface_hub import HfApi
from pathlib import Path
import shutil
import zipfile
import os
# %store -r

api = HfApi()

#@markdown #### This will be compressed to zip and  uploaded to datasets repo, leave it empty if not necessary
train_data_path = "/content/dreambooth/train_data" #@param {type :"string"}
#@markdown ##### `Nerd stuff, only if you want to save training logs`
logs_path = "/content/dreambooth/logs" #@param {type :"string"}
#@markdown #### Delete zip after upload
delete_zip = True #@param {type :"boolean"}

if project_name !="":
  tmp_dataset = "/content/dreambooth/"+project_name+"_dataset"
else:
  tmp_dataset = "/content/dreambooth/tmp_dataset"

dataset_zip = tmp_dataset + ".zip"

#@markdown #### Other Information
commit_message = "" #@param {type :"string"}

if not commit_message:
  commit_message = "feat: upload "+project_name+" dataset and logs"

os.makedirs(tmp_dataset)

def upload_dataset(dataset_paths, is_zip : bool):
  path_obj = Path(dataset_paths)
  dataset_name = path_obj.parts[-1]

  if is_zip:
    print(f"Uploading {dataset_name} to https://huggingface.co/datasets/"+datasets_repo)
    print(f"Please wait...")

    api.upload_file(
        path_or_fileobj=dataset_paths,
        path_in_repo=dataset_name,
        repo_id=datasets_repo,
        repo_type="dataset",
        commit_message=commit_message,
    )
    print(f"Upload success, located at https://huggingface.co/datasets/"+datasets_repo+"/blob/main/"+dataset_name+"\n")
  else:
    print(f"Uploading {dataset_name} to https://huggingface.co/datasets/"+datasets_repo)
    print(f"Please wait...")

    api.upload_folder(
        folder_path=dataset_paths,
        path_in_repo=dataset_name,
        repo_id=datasets_repo,
        repo_type="dataset",
        commit_message=commit_message,
        ignore_patterns=".ipynb_checkpoints",
    )
    print(f"Upload success, located at https://huggingface.co/datasets/"+datasets_repo+"/tree/main/"+dataset_name+"\n")
  
def zip_file(tmp):
    zipfiles = tmp + ".zip" 
    with zipfile.ZipFile(zipfiles, 'w') as zip:
      for tmp, dirs, files in os.walk(tmp):
          for file in files:
              zip.write(os.path.join(tmp, file))

def move(src_path, dst_path, is_metadata: bool):
  files_to_move = ["meta_cap.json", \
                   "meta_cap_dd.json", \
                   "meta_lat.json", \
                   "meta_clean.json", \
                   "meta_final.json"]

  if os.path.exists(src_path):
    shutil.move(src_path, dst_path)

  if is_metadata:
    parent_meta_path = os.path.dirname(src_path)

    for filename in os.listdir(parent_meta_path):
      file_path = os.path.join(parent_meta_path, filename)
      if filename in files_to_move:
        shutil.move(file_path, dst_path)

def upload():
  if train_data_path !="":
    move(train_data_path, tmp_dataset, False)
    zip_file(tmp_dataset)
    upload_dataset(dataset_zip, True)
    if delete_zip:
      os.remove(dataset_zip)
    
  if logs_path !="":
    upload_dataset(logs_path, False)

upload()

"""## 8.3. Commit using Git (Alternative)"""

#@title ### 8.3.1. Clone Repository

clone_model = True #@param {'type': 'boolean'}
clone_dataset = True #@param {'type': 'boolean'}

!git lfs install --skip-smudge
!export GIT_LFS_SKIP_SMUDGE=1

if clone_model:
  !git clone https://huggingface.co/{model_repo} /content/{model_name}
  
if clone_dataset:
  !git clone https://huggingface.co/datasets/{datasets_repo} /content/{dataset_name}

# Commented out IPython magic to ensure Python compatibility.
#@title ### 8.3.2. Commit using Git 
# %cd {root_dir}

#@markdown Tick which repo you want to commit
commit_model = True #@param {'type': 'boolean'}
commit_dataset = True #@param {'type': 'boolean'}

#@markdown Set **git commit identity**
email = "your-email-here" #@param {'type': 'string'}
name = "your-username-here" #@param {'type': 'string'}
#@markdown Set **commit message**
commit_message = "" #@param {type :"string"}

if not commit_message:
  commit_message = "feat: upload "+project_name+" lora model and dataset"

!git config --global user.email "{email}"
!git config --global user.name "{name}"

def commit(repo_folder, commit_message):
#   %cd {root_dir}/{repo_folder}
  !git lfs install
  !huggingface-cli lfs-enable-largefiles .
  !git add .
  !git commit -m "{commit_message}"
  !git push

commit(model_name, commit_message)
commit(dataset_name, commit_message)

"""# Become a supporter!
[![ko-fi](https://ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/linaqruf)
<a href="https://saweria.co/linaqruf"><img alt="Saweria" src="https://img.shields.io/badge/Saweria-7B3F00?style=for-the-badge&logo=ko-fi&logoColor=white"/></a>

"""
